{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['images/piano.png'](images/piano.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement \n",
    "\n",
    "Our task here is to take some existing music data then train a model using this existing data. The model has to learn the patterns in music that we humans enjoy. Once it learns this, the model should be able to generate new music for us. It cannot simply copy-paste from the training data. It has to understand the patterns of music to generate new music. We here are not expecting our model to generate new music which is of professional quality, but we want it to generate a decent quality music which should be melodious and good to hear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data\n",
    "\n",
    "The Input data we are using for developing the model is from **.mid** file. Let's gain some domain Knowledge. \n",
    "\n",
    "A MIDI file is not an audio recording. Rather, it is a set of instructions – for example, for pitch or tempo – and can use a thousand times less disk space than the equivalent recorded audio.\n",
    "\n",
    "To process these files we use **Music21**\n",
    "\n",
    "\n",
    "Music21 is a Python-based toolkit for computer-aided musicology.\n",
    "\n",
    "People use music21 to answer questions from musicology using computers, to study large datasets of music, to generate musical examples, to teach fundamentals of music theory, to edit musical notation, study music and the brain, and to compose music (both algorithmically and directly).\n",
    "\n",
    "> pip install music21\n",
    "\n",
    "**Importing the necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data from music notes\n",
    "\n",
    "The .mid files are stored in music_notes folder and we are implementing the below code to extract the data from each file and store it in notes list. **music21** library modules are utilized for parsing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing midi_songs\\0fithos.mid\n",
      "Parsing midi_songs\\8.mid\n",
      "Parsing midi_songs\\ahead_on_our_way_piano.mid\n",
      "Parsing midi_songs\\AT.mid\n",
      "Parsing midi_songs\\balamb.mid\n",
      "Parsing midi_songs\\bcm.mid\n",
      "Parsing midi_songs\\BlueStone_LastDungeon.mid\n",
      "Parsing midi_songs\\braska.mid\n",
      "Parsing midi_songs\\caitsith.mid\n",
      "Parsing midi_songs\\Cids.mid\n",
      "Parsing midi_songs\\cosmo.mid\n",
      "Parsing midi_songs\\costadsol.mid\n",
      "Parsing midi_songs\\dayafter.mid\n",
      "Parsing midi_songs\\decisive.mid\n",
      "Parsing midi_songs\\dontbeafraid.mid\n",
      "Parsing midi_songs\\DOS.mid\n",
      "Parsing midi_songs\\electric_de_chocobo.mid\n",
      "Parsing midi_songs\\Eternal_Harvest.mid\n",
      "Parsing midi_songs\\EyesOnMePiano.mid\n",
      "Parsing midi_songs\\ff11_awakening_piano.mid\n",
      "Parsing midi_songs\\ff1battp.mid\n",
      "Parsing midi_songs\\FF3_Battle_(Piano).mid\n",
      "Parsing midi_songs\\FF3_Third_Phase_Final_(Piano).mid\n",
      "Parsing midi_songs\\ff4-airship.mid\n",
      "Parsing midi_songs\\Ff4-BattleLust.mid\n",
      "Parsing midi_songs\\ff4-fight1.mid\n",
      "Parsing midi_songs\\ff4-town.mid\n",
      "Parsing midi_songs\\FF4.mid\n",
      "Parsing midi_songs\\ff4pclov.mid\n",
      "Parsing midi_songs\\ff4_piano_collections-main_theme.mid\n",
      "Parsing midi_songs\\FF6epitaph_piano.mid\n",
      "Parsing midi_songs\\ff6shap.mid\n",
      "Parsing midi_songs\\Ff7-Cinco.mid\n",
      "Parsing midi_songs\\Ff7-Jenova_Absolute.mid\n",
      "Parsing midi_songs\\ff7-mainmidi.mid\n",
      "Parsing midi_songs\\Ff7-One_Winged.mid\n",
      "Parsing midi_songs\\ff7themep.mid\n",
      "Parsing midi_songs\\ff8-lfp.mid\n",
      "Parsing midi_songs\\FF8_Shuffle_or_boogie_pc.mid\n",
      "Parsing midi_songs\\FFIII_Edgar_And_Sabin_Piano.mid\n",
      "Parsing midi_songs\\FFIXQuMarshP.mid\n",
      "Parsing midi_songs\\FFIX_Piano.mid\n",
      "Parsing midi_songs\\FFVII_BATTLE.mid\n",
      "Parsing midi_songs\\FFX_-_Ending_Theme_(Piano_Version)_-_by_Angel_FF.mid\n",
      "Parsing midi_songs\\Fiend_Battle_(Piano).mid\n",
      "Parsing midi_songs\\Fierce_Battle_(Piano).mid\n",
      "Parsing midi_songs\\figaro.mid\n",
      "Parsing midi_songs\\Finalfantasy5gilgameshp.mid\n",
      "Parsing midi_songs\\Finalfantasy6fanfarecomplete.mid\n",
      "Parsing midi_songs\\Final_Fantasy_7_-_Judgement_Day_Piano.mid\n",
      "Parsing midi_songs\\Final_Fantasy_Matouyas_Cave_Piano.mid\n",
      "Parsing midi_songs\\fortresscondor.mid\n",
      "Parsing midi_songs\\Fyw_piano.mid\n",
      "Parsing midi_songs\\gerudo.mid\n",
      "Parsing midi_songs\\goldsaucer.mid\n",
      "Parsing midi_songs\\Gold_Silver_Rival_Battle.mid\n",
      "Parsing midi_songs\\great_war.mid\n",
      "Parsing midi_songs\\HighwindTakestotheSkies.mid\n",
      "Parsing midi_songs\\In_Zanarkand.mid\n",
      "Parsing midi_songs\\JENOVA.mid\n",
      "Parsing midi_songs\\Kingdom_Hearts_Dearly_Beloved.mid\n",
      "Parsing midi_songs\\Kingdom_Hearts_Traverse_Town.mid\n",
      "Parsing midi_songs\\Life_Stream.mid\n",
      "Parsing midi_songs\\lurk_in_dark.mid\n",
      "Parsing midi_songs\\mining.mid\n",
      "Parsing midi_songs\\Oppressed.mid\n",
      "Parsing midi_songs\\OTD5YA.mid\n",
      "Parsing midi_songs\\path_of_repentance.mid\n",
      "Parsing midi_songs\\pkelite4.mid\n",
      "Parsing midi_songs\\Rachel_Piano_tempofix.mid\n",
      "Parsing midi_songs\\redwings.mid\n",
      "Parsing midi_songs\\relmstheme-piano.mid\n",
      "Parsing midi_songs\\roseofmay-piano.mid\n",
      "Parsing midi_songs\\rufus.mid\n",
      "Parsing midi_songs\\Rydia_pc.mid\n",
      "Parsing midi_songs\\sandy.mid\n",
      "Parsing midi_songs\\sera_.mid\n",
      "Parsing midi_songs\\sobf.mid\n",
      "Parsing midi_songs\\Still_Alive-1.mid\n",
      "Parsing midi_songs\\Suteki_Da_Ne_(Piano_Version).mid\n",
      "Parsing midi_songs\\thenightmarebegins.mid\n",
      "Parsing midi_songs\\thoughts.mid\n",
      "Parsing midi_songs\\tifap.mid\n",
      "Parsing midi_songs\\tpirtsd-piano.mid\n",
      "Parsing midi_songs\\traitor.mid\n",
      "Parsing midi_songs\\ultimafro.mid\n",
      "Parsing midi_songs\\ultros.mid\n",
      "Parsing midi_songs\\VincentPiano.mid\n",
      "Parsing midi_songs\\ViviinAlexandria.mid\n",
      "Parsing midi_songs\\waltz_de_choco.mid\n",
      "Parsing midi_songs\\Zelda_Overworld.mid\n",
      "Parsing midi_songs\\z_aeristhemepiano.mid\n"
     ]
    }
   ],
   "source": [
    "notes=[]\n",
    "\n",
    "for file in glob.glob('midi_songs/*.mid'):\n",
    "    midi=converter.parse(file)\n",
    "    print(\"Parsing %s\" % file)\n",
    "    notes_to_parse=None\n",
    "    \n",
    "    parts=instrument.partitionByInstrument(midi)\n",
    "    \n",
    "    if parts:\n",
    "        notes_to_parse=parts.parts[0].recurse()\n",
    "    else:\n",
    "        notes_to_parse=midi.flat.notes\n",
    "        \n",
    "    for element in notes_to_parse:\n",
    "        if isinstance(element,note.Note):\n",
    "            notes.append(str(element.pitch))\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            notes.append('.'.join(str(n) for n in element.normalOrder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E2',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '11.4',\n",
       " '4.9',\n",
       " '11.4',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '4.9',\n",
       " '0.4',\n",
       " 'E2',\n",
       " '4.9']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**saving the data in notes file for futher reuse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "\n",
    "The Neural network we are creating has LSTM Layers after Input Layer. we need to prepare our data as per it's requirement. At present our data is just a list of notes. We need to create a list of sequences as features and  list of their next note as Target variable\n",
    "\n",
    "Ex: \n",
    "\n",
    "A sequence with increment 10\n",
    "\n",
    "10,20,30,40,50,60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If we take 3 steps and and our data has single feature\n",
    "\n",
    "|x  | y           \n",
    "--- | --- \n",
    "| 10 20 30|40\n",
    "|20 30 40 |50\n",
    "|30 40 50  | 60\n",
    "\n",
    "and If we give 40,50,60 our model has to predict output as 70.\n",
    "\n",
    "\n",
    "\n",
    "Our data example:\n",
    "\n",
    "suppose we have only four notes. Let them be A, B, C, D\n",
    "\n",
    "and input sequence is AABACCDB\n",
    "\n",
    "we will create dictionary mapping them to integers\n",
    "\n",
    "|A|B|C|D\n",
    "---|---|---|---\n",
    "0|1|2|3\n",
    "\n",
    "Now our input sequence became 00102231\n",
    "\n",
    "Now we will create a list of sequences X\n",
    "\n",
    "|x  | y           \n",
    "--- | --- \n",
    "| 0 0 1|0\n",
    "|0 1 0 |2\n",
    "|1 0 2  |2\n",
    "|0 2 2 | 3  \n",
    "|2  2  3|1 \n",
    "\n",
    "Now Y is one hot encoded.\n",
    "\n",
    "\n",
    "|x  | y           \n",
    "--- | --- \n",
    "| 0 0 1|0 0 0 0\n",
    "|0 1 0 |0 0 1 0\n",
    "|1 0 2  |0 0 1 0\n",
    "|0 2 3 | 0 0 0 1\n",
    "|2 3 4|0 1 0 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=100\n",
    "pitchnames=sorted(set(item for item in notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a dictionary mapping the pitched to integers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab=len(set(notes))\n",
    "\n",
    "sequence_length = 100\n",
    "# get all pitch names\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "# create a dictionary to map pitches to integers\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "network_input = []\n",
    "network_output = []\n",
    "# create input sequences and the corresponding outputs\n",
    "for i in range(0, len(notes) - sequence_length, 1):\n",
    "    sequence_in = notes[i:i + sequence_length]\n",
    "    sequence_out = notes[i + sequence_length]\n",
    "    network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    network_output.append(note_to_int[sequence_out])\n",
    "n_patterns = len(network_input)\n",
    "# reshape the input into a format compatible with LSTM layers\n",
    "network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "# normalize input\n",
    "network_input = network_input / float(n_vocab)\n",
    "network_output = np_utils.to_categorical(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57077, 100, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57077, 358)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "\n",
    "Our model will take 100 notes and predict the 101 one and and now 102 note is produced by feeding 2-101 notes and so on...\n",
    "\n",
    "Key Layer for our model is LSTM. Let's know a little bit about it.\n",
    "\n",
    "### LSTM\n",
    "\n",
    "![](lstm_in.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Forget Gate\n",
    "\n",
    "**Bob is nice person but Alice is evil**\n",
    "\n",
    "As soon as the first full stop after “person” is encountered, the forget gate realizes that there may be a change of context in the next sentence. As a result of this, the subject of the sentence is forgotten and the place for the subject is vacated. And when we start speaking about “Dan” this position of the subject is allocated to “Dan”. This process of forgetting the subject is brought about by the forget gate.\n",
    "\n",
    "2. Input Gate\n",
    "\n",
    "**Bob knows swimming. He told me over the phone that he served for navy for 4 years**\n",
    "\n",
    "Now the important information here is that “Bob” knows swimming and that he has served the Navy for four years. This can be added to the cell state, however, the fact that he told all this over the phone is a less important fact and can be ignored. This process of adding some new information can be done via the input gate.\n",
    "\n",
    "3. Output Gate\n",
    "\n",
    "**Bob fought single handedly with the enemy and died for his country. For his contributions brave____________**\n",
    "\n",
    "In this phrase, there could be a number of options for the empty space. But we know that the current input of ‘brave’, is an adjective that is used to describe a noun. Thus, whatever word follows, has a strong tendency of being a noun. And thus, Bob could be an apt output.\n",
    "\n",
    "This job of selecting useful information from the current cell state and showing it out as an output is done via the output gate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    512,\n",
    "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "57077/57077 [==============================] - 584s 10ms/step - loss: 4.7701\n",
      "Epoch 2/100\n",
      "57077/57077 [==============================] - 573s 10ms/step - loss: 4.7976\n",
      "Epoch 3/100\n",
      "57077/57077 [==============================] - 571s 10ms/step - loss: 4.7119\n",
      "Epoch 4/100\n",
      "57077/57077 [==============================] - 569s 10ms/step - loss: 4.7356\n",
      "Epoch 5/100\n",
      "57077/57077 [==============================] - 568s 10ms/step - loss: 4.7034\n",
      "Epoch 6/100\n",
      "57077/57077 [==============================] - 568s 10ms/step - loss: 4.7244\n",
      "Epoch 7/100\n",
      "57077/57077 [==============================] - 565s 10ms/step - loss: 4.7030\n",
      "Epoch 8/100\n",
      "57077/57077 [==============================] - 567s 10ms/step - loss: 4.6946\n",
      "Epoch 9/100\n",
      "57077/57077 [==============================] - 566s 10ms/step - loss: 4.6919\n",
      "Epoch 10/100\n",
      "57077/57077 [==============================] - 565s 10ms/step - loss: 4.6375\n",
      "Epoch 11/100\n",
      "57077/57077 [==============================] - 565s 10ms/step - loss: 4.5706\n",
      "Epoch 12/100\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 4.5414\n",
      "Epoch 13/100\n",
      "38528/57077 [===================>..........] - ETA: 3:03 - loss: 4.6084"
     ]
    }
   ],
   "source": [
    "filepath='my_weights-improvement={epoch:02d}-{loss:.4f}-bigger.hdf5'\n",
    "checkpoint=ModelCheckpoint(filepath,monitor='loss',verbose=0,save_best_only=True,mode='min')\n",
    "callbacks_list=[checkpoint]\n",
    "\n",
    "model.fit(net_input,net_output,epochs=100,batch_size=128,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reloading the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('my_weights-improvement=100-0.2741-bigger.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['images/piano.png'](music_generator.h5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 358)               92006     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 358)               0         \n",
      "=================================================================\n",
      "Total params: 5,474,406\n",
      "Trainable params: 5,474,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the already trained one for futher Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2689\n",
      "Epoch 2/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2668\n",
      "Epoch 3/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.2543\n",
      "Epoch 4/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.2554\n",
      "Epoch 5/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2508\n",
      "Epoch 6/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.2518\n",
      "Epoch 7/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2478\n",
      "Epoch 8/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.2414\n",
      "Epoch 9/200\n",
      "57077/57077 [==============================] - 564s 10ms/step - loss: 0.2404\n",
      "Epoch 10/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2396\n",
      "Epoch 11/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2312\n",
      "Epoch 12/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2267\n",
      "Epoch 13/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.2219\n",
      "Epoch 14/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2239\n",
      "Epoch 15/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2198\n",
      "Epoch 16/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.2158\n",
      "Epoch 17/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2112\n",
      "Epoch 18/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.2125\n",
      "Epoch 19/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2071\n",
      "Epoch 20/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.2042\n",
      "Epoch 21/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.2061\n",
      "Epoch 22/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.2031\n",
      "Epoch 23/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.2009\n",
      "Epoch 24/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.2001\n",
      "Epoch 25/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1974\n",
      "Epoch 26/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1915\n",
      "Epoch 27/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1932\n",
      "Epoch 28/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1932\n",
      "Epoch 29/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1868\n",
      "Epoch 30/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1876\n",
      "Epoch 31/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1874\n",
      "Epoch 32/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1885\n",
      "Epoch 33/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1859\n",
      "Epoch 34/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1798\n",
      "Epoch 35/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1852\n",
      "Epoch 36/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1822\n",
      "Epoch 37/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1816\n",
      "Epoch 38/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1803\n",
      "Epoch 39/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1771\n",
      "Epoch 40/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1728\n",
      "Epoch 41/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1737\n",
      "Epoch 42/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1758\n",
      "Epoch 43/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1714\n",
      "Epoch 44/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1721\n",
      "Epoch 45/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1713\n",
      "Epoch 46/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1700\n",
      "Epoch 47/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1653\n",
      "Epoch 48/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1686\n",
      "Epoch 49/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1721\n",
      "Epoch 50/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1642\n",
      "Epoch 51/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1642\n",
      "Epoch 52/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1648\n",
      "Epoch 53/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1574\n",
      "Epoch 54/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1603\n",
      "Epoch 55/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1658\n",
      "Epoch 56/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1582\n",
      "Epoch 57/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1589\n",
      "Epoch 58/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1620\n",
      "Epoch 59/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1550\n",
      "Epoch 60/200\n",
      "57077/57077 [==============================] - 559s 10ms/step - loss: 0.1588\n",
      "Epoch 61/200\n",
      "57077/57077 [==============================] - 554s 10ms/step - loss: 0.1584\n",
      "Epoch 62/200\n",
      "57077/57077 [==============================] - 570s 10ms/step - loss: 0.1537\n",
      "Epoch 63/200\n",
      "57077/57077 [==============================] - 574s 10ms/step - loss: 0.1556\n",
      "Epoch 64/200\n",
      "57077/57077 [==============================] - 569s 10ms/step - loss: 0.1540\n",
      "Epoch 65/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1529\n",
      "Epoch 66/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1564\n",
      "Epoch 67/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1526\n",
      "Epoch 68/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1574\n",
      "Epoch 69/200\n",
      "57077/57077 [==============================] - 563s 10ms/step - loss: 0.1565\n",
      "Epoch 70/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1510\n",
      "Epoch 71/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1495\n",
      "Epoch 72/200\n",
      "57077/57077 [==============================] - 561s 10ms/step - loss: 0.1489\n",
      "Epoch 73/200\n",
      "57077/57077 [==============================] - 562s 10ms/step - loss: 0.1484\n",
      "Epoch 74/200\n",
      "39936/57077 [===================>..........] - ETA: 2:48 - loss: 0.1467"
     ]
    }
   ],
   "source": [
    "filepath='weights_folder/my_weights-improvement={epoch:02d}-{loss:.4f}-bigger.hdf5'\n",
    "checkpoint=ModelCheckpoint(filepath,monitor='loss',verbose=0,save_best_only=True,mode='min')\n",
    "callbacks_list=[checkpoint]\n",
    "model.fit(net_input,net_output,epochs=200,batch_size=128,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model and Model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('music_generator.h5')\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random sequence from the input as a starting point for the prediction\n",
    "start = np.random.randint(0, len(network_input)-1)\n",
    "start\n",
    "\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "pattern = network_input[start]\n",
    "prediction_output = []\n",
    "\n",
    "pattern=list(pattern)\n",
    "pattern\n",
    "\n",
    "for note_index in range(500):\n",
    "    prediction_input=np.reshape(pattern,(1,len(pattern),1))\n",
    "    prediction_input=prediction_input/float(n_vocab)\n",
    "    \n",
    "    prediction=model.predict(prediction_input,verbose=0)\n",
    "    \n",
    "    index=np.argmax(prediction)\n",
    "    result=int_to_note[index]\n",
    "    \n",
    "    prediction_output.append(result)\n",
    "    \n",
    "    pattern.append(index)\n",
    "    \n",
    "    pattern=pattern[1:len(pattern)]\n",
    "\n",
    "prediction_output\n",
    "\n",
    "from music21 import instrument, note, stream, chord\n",
    "\n",
    "offset = 0\n",
    "output_notes = []\n",
    "\n",
    "# create note and chord objects based on the values generated by the model\n",
    "for pattern in prediction_output:\n",
    "    # pattern is a chord\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # pattern is a note\n",
    "    else:\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    # increase offset each iteration so that notes do not stack\n",
    "    offset += 0.5\n",
    "\n",
    "midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "midi_stream.write('midi', fp='new_test_output_168.mid')\n",
    "\n",
    "\n",
    "prediction_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
